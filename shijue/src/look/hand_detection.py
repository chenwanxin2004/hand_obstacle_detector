"""
æ‰‹éƒ¨å…³èŠ‚æ£€æµ‹æ¨¡å—
"""

import cv2
import mediapipe as mp
import numpy as np
from typing import List, Tuple, Optional
from .yolo_detector import YOLODetector


class HandDetector:
    """æ‰‹éƒ¨æ£€æµ‹å™¨ç±»"""
    
    def __init__(self, yolo_model_path: str = None):
        """
        åˆå§‹åŒ–æ‰‹éƒ¨æ£€æµ‹å™¨
        
        Args:
            yolo_model_path: YOLOæ¨¡å‹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        """
        # åˆå§‹åŒ–MediaPipeæ‰‹éƒ¨æ£€æµ‹
        self.mp_hands = mp.solutions.hands
        self.mp_drawing = mp.solutions.drawing_utils
        self.mp_drawing_styles = mp.solutions.drawing_styles
        
        # åˆå§‹åŒ–æ‰‹éƒ¨æ£€æµ‹å™¨
        self.hands = self.mp_hands.Hands(
            model_complexity=0,
            min_detection_confidence=0.5,
            min_tracking_confidence=0.5,
            max_num_hands=2  # æœ€å¤šæ£€æµ‹2åªæ‰‹
        )
        
        # åˆå§‹åŒ–YOLOæ£€æµ‹å™¨ï¼ˆå¦‚æœæä¾›è·¯å¾„ï¼‰
        self.yolo_detector = None
        if yolo_model_path:
            try:
                self.yolo_detector = YOLODetector(yolo_model_path)
            except Exception as e:
                print(f"âš ï¸ YOLOæ£€æµ‹å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def process_frame(self, image: np.ndarray) -> Tuple[np.ndarray, List, List]:
        """
        å¤„ç†å•å¸§å›¾åƒ
        
        Args:
            image: è¾“å…¥å›¾åƒ (BGRæ ¼å¼)
            
        Returns:
            å¤„ç†åçš„å›¾åƒã€æ‰‹éƒ¨å…³é”®ç‚¹ã€YOLOæ£€æµ‹ç»“æœ
        """
        # è½¬æ¢ä¸ºRGBæ ¼å¼ï¼ˆMediaPipeéœ€è¦ï¼‰
        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
        
        # æ‰‹éƒ¨æ£€æµ‹
        results = self.hands.process(image_rgb)
        
        # è½¬æ¢å›BGRæ ¼å¼
        image_bgr = cv2.cvtColor(image_rgb, cv2.COLOR_RGB2BGR)
        
        hand_landmarks_list = []
        yolo_detections = []
        
        # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹
        if results.multi_hand_landmarks:
            for hand_landmarks in results.multi_hand_landmarks:
                # ç»˜åˆ¶æ‰‹éƒ¨å…³é”®ç‚¹å’Œè¿æ¥çº¿
                self.mp_drawing.draw_landmarks(
                    image_bgr,
                    hand_landmarks,
                    self.mp_hands.HAND_CONNECTIONS,
                    self.mp_drawing_styles.get_default_hand_landmarks_style(),
                    self.mp_drawing_styles.get_default_hand_connections_style()
                )
                
                # æå–å…³é”®ç‚¹åæ ‡
                landmarks = self._extract_landmarks(hand_landmarks, image_bgr.shape)
                hand_landmarks_list.append(landmarks)
        
        # YOLOç›®æ ‡æ£€æµ‹
        if self.yolo_detector:
            yolo_detections = self.yolo_detector.detect(image_bgr)
            
            # ç»˜åˆ¶YOLOæ£€æµ‹æ¡†
            for detection in yolo_detections:
                x1, y1, x2, y2, conf, class_id = detection
                cv2.rectangle(image_bgr, (x1, y1), (x2, y2), (0, 255, 0), 2)
                cv2.putText(
                    image_bgr, 
                    f'Class: {class_id}, Conf: {conf:.2f}', 
                    (x1, y1 - 10), 
                    cv2.FONT_HERSHEY_SIMPLEX, 
                    0.5, 
                    (0, 255, 0), 
                    2
                )
        
        return image_bgr, hand_landmarks_list, yolo_detections
    
    def _extract_landmarks(self, hand_landmarks, image_shape: Tuple[int, int, int]) -> List[Tuple[int, int]]:
        """
        æå–æ‰‹éƒ¨å…³é”®ç‚¹åæ ‡
        
        Args:
            hand_landmarks: MediaPipeæ‰‹éƒ¨å…³é”®ç‚¹
            image_shape: å›¾åƒå°ºå¯¸ (height, width, channels)
            
        Returns:
            å…³é”®ç‚¹åæ ‡åˆ—è¡¨ [(x, y), ...]
        """
        height, width = image_shape[:2]
        landmarks = []
        
        for landmark in hand_landmarks.landmark:
            x = int(landmark.x * width)
            y = int(landmark.y * height)
            landmarks.append((x, y))
        
        return landmarks
    
    def get_hand_gesture(self, landmarks: List[Tuple[int, int]]) -> str:
        """
        è¯†åˆ«æ‰‹éƒ¨æ‰‹åŠ¿ï¼ˆç®€å•ç¤ºä¾‹ï¼‰
        
        Args:
            landmarks: æ‰‹éƒ¨å…³é”®ç‚¹åæ ‡
            
        Returns:
            æ‰‹åŠ¿åç§°
        """
        if len(landmarks) < 21:  # MediaPipeæ‰‹éƒ¨æ£€æµ‹æœ‰21ä¸ªå…³é”®ç‚¹
            return "Unknown"
        
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„æ‰‹åŠ¿è¯†åˆ«é€»è¾‘
        # ç®€å•ç¤ºä¾‹ï¼šæ£€æµ‹æ‰‹æŒæ˜¯å¦å¼ å¼€
        thumb_tip = landmarks[4]
        index_tip = landmarks[8]
        middle_tip = landmarks[12]
        ring_tip = landmarks[16]
        pinky_tip = landmarks[20]
        
        # è®¡ç®—æ‰‹æŒ‡æ˜¯å¦ä¼¸å±•ï¼ˆç®€åŒ–é€»è¾‘ï¼‰
        # å®é™…åº”ç”¨ä¸­éœ€è¦æ›´å¤æ‚çš„å‡ ä½•è®¡ç®—
        
        return "Hand Detected"
    
    def release(self):
        """é‡Šæ”¾èµ„æº"""
        if self.hands:
            self.hands.close()





def process_camera(
    camera_id: int = 0, 
    yolo_model_path: str = None,
    save_frames: bool = False
) -> None:
    """
    å¤„ç†æ‘„åƒå¤´å®æ—¶æµ
    
    Args:
        camera_id: æ‘„åƒå¤´IDï¼ˆé€šå¸¸0æ˜¯é»˜è®¤æ‘„åƒå¤´ï¼‰
        yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
        save_frames: æ˜¯å¦ä¿å­˜æ£€æµ‹å¸§
    """
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = HandDetector(yolo_model_path)
    
    # æ‰“å¼€æ‘„åƒå¤´
    cap = cv2.VideoCapture(camera_id)
    
    if not cap.isOpened():
        print(f"âŒ æ— æ³•æ‰“å¼€æ‘„åƒå¤´: {camera_id}")
        return
    
    print(f"ğŸ“¹ å¼€å§‹æ‘„åƒå¤´æ£€æµ‹ (ID: {camera_id})")
    print("ğŸ’¡ æŒ‰ 'q' é€€å‡ºï¼ŒæŒ‰ 's' ä¿å­˜å½“å‰å¸§")
    if save_frames:
        print("ğŸ’¾ è‡ªåŠ¨ä¿å­˜æ¨¡å¼å·²å¯ç”¨")
    
    frame_count = 0
    try:
        while True:
            success, frame = cap.read()
            if not success:
                print("âš ï¸ æ— æ³•è¯»å–æ‘„åƒå¤´å¸§")
                break
            
            # å¤„ç†å¸§
            processed_frame, hand_landmarks, yolo_detections = detector.process_frame(frame)
            
            # æ˜¾ç¤ºå¤„ç†ç»“æœ
            cv2.imshow('Real-time Hand Detection + YOLO', processed_frame)
            
            # è‡ªåŠ¨ä¿å­˜å¸§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            if save_frames and frame_count % 30 == 0:  # æ¯30å¸§ä¿å­˜ä¸€æ¬¡
                filename = f"auto_capture_{frame_count:06d}.jpg"
                cv2.imwrite(filename, processed_frame)
                print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜å¸§: {filename}")
            
            # é”®ç›˜æ§åˆ¶
            key = cv2.waitKey(1) & 0xFF
            if key == ord('q'):
                break
            elif key == ord('s'):
                # æ‰‹åŠ¨ä¿å­˜å½“å‰å¸§
                filename = f"manual_capture_{cv2.getTickCount()}.jpg"
                cv2.imwrite(filename, processed_frame)
                print(f"ğŸ’¾ æ‰‹åŠ¨ä¿å­˜å¸§: {filename}")
            
            frame_count += 1
    
    except KeyboardInterrupt:
        print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
    
    finally:
        # æ¸…ç†èµ„æº
        cap.release()
        cv2.destroyAllWindows()
        detector.release()
        print(f"âœ… æ‘„åƒå¤´æ£€æµ‹ç»“æŸï¼Œå…±å¤„ç† {frame_count} å¸§")


def process_realsense(
    yolo_model_path: str = None,
    save_frames: bool = False
) -> None:
    """
    å¤„ç†Intel RealSense D435æ‘„åƒå¤´å®æ—¶æµ
    
    Args:
        yolo_model_path: YOLOæ¨¡å‹è·¯å¾„
        save_frames: æ˜¯å¦ä¿å­˜æ£€æµ‹å¸§
    """
    try:
        import pyrealsense2 as rs
    except ImportError:
        print("âŒ æœªå®‰è£…pyrealsense2åº“")
        print("è¯·è¿è¡Œ: pip install pyrealsense2")
        return
    
    # åˆå§‹åŒ–æ£€æµ‹å™¨
    detector = HandDetector(yolo_model_path)
    
    # é…ç½®RealSenseç®¡é“
    pipeline = rs.pipeline()
    config = rs.config()
    
    # å¯ç”¨å½©è‰²æµå’Œæ·±åº¦æµ
    config.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)
    config.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)
    
    try:
        # å¯åŠ¨ç®¡é“
        profile = pipeline.start(config)
        print("âœ… Intel RealSense D435 æ‘„åƒå¤´è¿æ¥æˆåŠŸ")
        
        # è·å–æ·±åº¦ä¼ æ„Ÿå™¨
        depth_sensor = profile.get_device().first_depth_sensor()
        depth_scale = depth_sensor.get_depth_scale()
        print(f"ğŸ“ æ·±åº¦æ¯”ä¾‹å› å­: {depth_scale}")
        
        # åˆ›å»ºæ·±åº¦åˆ°å½©è‰²çš„å¯¹é½å™¨
        align = rs.align(rs.stream.color)
        
        print("ğŸ“¹ å¼€å§‹RealSenseæ‘„åƒå¤´æ£€æµ‹")
        print("ğŸ’¡ æŒ‰ 'q' é€€å‡ºï¼ŒæŒ‰ 's' ä¿å­˜å½“å‰å¸§")
        if save_frames:
            print("ğŸ’¾ è‡ªåŠ¨ä¿å­˜æ¨¡å¼å·²å¯ç”¨")
        
        frame_count = 0
        
        try:
            while True:
                # ç­‰å¾…æ–°çš„å¸§
                frames = pipeline.wait_for_frames()
                
                # å¯¹é½æ·±åº¦å¸§åˆ°å½©è‰²å¸§
                aligned_frames = align.process(frames)
                
                # è·å–å¯¹é½åçš„å¸§
                color_frame = aligned_frames.get_color_frame()
                depth_frame = aligned_frames.get_depth_frame()
                
                if not color_frame or not depth_frame:
                    continue
                
                # è½¬æ¢ä¸ºnumpyæ•°ç»„
                color_image = np.asanyarray(color_frame.get_data())
                depth_image = np.asanyarray(depth_frame.get_data())
                
                # å¤„ç†å½©è‰²å¸§è¿›è¡Œæ‰‹éƒ¨æ£€æµ‹
                processed_frame, hand_landmarks, yolo_detections = detector.process_frame(color_image)
                
                # åœ¨æ·±åº¦å›¾åƒä¸Šç»˜åˆ¶æ‰‹éƒ¨æ£€æµ‹ç»“æœ
                depth_colormap = cv2.applyColorMap(cv2.convertScaleAbs(depth_image, alpha=0.03), cv2.COLORMAP_JET)
                
                # å¦‚æœæœ‰æ‰‹éƒ¨æ£€æµ‹ï¼Œåœ¨æ·±åº¦å›¾ä¸Šä¹Ÿç»˜åˆ¶
                if hand_landmarks:
                    for landmarks in hand_landmarks:
                        for landmark in landmarks:
                            x, y = int(landmark[0]), int(landmark[1])
                            if 0 <= x < depth_colormap.shape[1] and 0 <= y < depth_colormap.shape[0]:
                                depth_value = depth_image[y, x]
                                if depth_value > 0:
                                    cv2.circle(depth_colormap, (x, y), 3, (0, 255, 0), -1)
                                    cv2.putText(depth_colormap, f"{depth_value}mm", (x+5, y-5), 
                                               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1)
                
                # æ˜¾ç¤ºå¤„ç†ç»“æœ
                cv2.imshow('RealSense - Hand Detection + YOLO', processed_frame)
                cv2.imshow('RealSense - Depth Map', depth_colormap)
                
                # è‡ªåŠ¨ä¿å­˜å¸§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if save_frames and frame_count % 30 == 0:  # æ¯30å¸§ä¿å­˜ä¸€æ¬¡
                    color_filename = f"realsense_color_{frame_count:06d}.jpg"
                    depth_filename = f"realsense_depth_{frame_count:06d}.jpg"
                    cv2.imwrite(color_filename, processed_frame)
                    cv2.imwrite(depth_filename, depth_colormap)
                    print(f"ğŸ’¾ è‡ªåŠ¨ä¿å­˜å¸§: {color_filename}, {depth_filename}")
                
                # é”®ç›˜æ§åˆ¶
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    break
                elif key == ord('s'):
                    # æ‰‹åŠ¨ä¿å­˜å½“å‰å¸§
                    timestamp = cv2.getTickCount()
                    color_filename = f"realsense_manual_color_{timestamp}.jpg"
                    depth_filename = f"realsense_manual_depth_{timestamp}.jpg"
                    cv2.imwrite(color_filename, processed_frame)
                    cv2.imwrite(depth_filename, depth_colormap)
                    print(f"ğŸ’¾ æ‰‹åŠ¨ä¿å­˜å¸§: {color_filename}, {depth_filename}")
                
                frame_count += 1
                
        except KeyboardInterrupt:
            print("\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­")
        
        finally:
            # åœæ­¢ç®¡é“
            pipeline.stop()
            cv2.destroyAllWindows()
            detector.release()
            print(f"âœ… RealSenseæ‘„åƒå¤´æ£€æµ‹ç»“æŸï¼Œå…±å¤„ç† {frame_count} å¸§")
    
    except Exception as e:
        print(f"âŒ RealSenseæ‘„åƒå¤´è¿æ¥å¤±è´¥: {e}")
        print("è¯·æ£€æŸ¥æ‘„åƒå¤´è¿æ¥å’Œé©±åŠ¨å®‰è£…")
        pipeline.stop()
        detector.release()
